<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>NeuS Paper 细读 - TosakaUCW</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="TosakaUCW"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="TosakaUCW"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="PaperAbstract 我们提出了一种新颖的神经表面重建方法，称为NeuS，用于从2D图像输入中高保真重建对象和场景。现有的神经表面重建方法，如DVR [Niemeyer et al., 2020] 和IDR [Yariv et al., 2020]，需要前景掩码作为监督，容易陷入局部最小值，因此在重建具有严重自遮挡或薄结构的对象时存在困难。与此同时，最近的用于新视角合成的神经方法，如NeRF"><meta property="og:type" content="blog"><meta property="og:title" content="NeuS Paper 细读"><meta property="og:url" content="https://tosakaucw.github.io/neus-paper-%E7%BB%86%E8%AF%BB/"><meta property="og:site_name" content="TosakaUCW"><meta property="og:description" content="PaperAbstract 我们提出了一种新颖的神经表面重建方法，称为NeuS，用于从2D图像输入中高保真重建对象和场景。现有的神经表面重建方法，如DVR [Niemeyer et al., 2020] 和IDR [Yariv et al., 2020]，需要前景掩码作为监督，容易陷入局部最小值，因此在重建具有严重自遮挡或薄结构的对象时存在困难。与此同时，最近的用于新视角合成的神经方法，如NeRF"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://tosakaucw.github.io/img/og_image.png"><meta property="article:published_time" content="2024-07-03T16:00:53.000Z"><meta property="article:modified_time" content="2024-07-04T08:56:55.332Z"><meta property="article:author" content="TosakaUCW"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="https://tosakaucw.github.io/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://tosakaucw.github.io/neus-paper-%E7%BB%86%E8%AF%BB/"},"headline":"NeuS Paper 细读","image":["https://tosakaucw.github.io/img/og_image.png"],"datePublished":"2024-07-03T16:00:53.000Z","dateModified":"2024-07-04T08:56:55.332Z","author":{"@type":"Person","name":"TosakaUCW"},"publisher":{"@type":"Organization","name":"TosakaUCW","logo":{"@type":"ImageObject","url":"https://tosakaucw.github.io/img/logo.svg"}},"description":"PaperAbstract 我们提出了一种新颖的神经表面重建方法，称为NeuS，用于从2D图像输入中高保真重建对象和场景。现有的神经表面重建方法，如DVR [Niemeyer et al., 2020] 和IDR [Yariv et al., 2020]，需要前景掩码作为监督，容易陷入局部最小值，因此在重建具有严重自遮挡或薄结构的对象时存在困难。与此同时，最近的用于新视角合成的神经方法，如NeRF"}</script><link rel="canonical" href="https://tosakaucw.github.io/neus-paper-%E7%BB%86%E8%AF%BB/"><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link data-pjax rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@11.7.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Fira+Code&amp;family=Microsoft+Yahei:wght@400;700"><link data-pjax rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.2.0"></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="TosakaUCW" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a><a class="navbar-item" href="/links">Links</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/TosakaUCW"><i class="fab fa-github"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="Catalogue" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-9-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><i class="far fa-calendar-alt"> </i><time dateTime="2024-07-03T16:00:53.000Z" title="2024-07-03T16:00:53.000Z">2024-07-04</time></span><span class="level-item is-hidden-mobile"><i class="far fa-calendar-check"> </i><time dateTime="2024-07-04T08:56:55.332Z" title="2024-07-04T08:56:55.332Z">2024-07-04</time></span></div></div><h1 class="title is-3 is-size-4-mobile">NeuS Paper 细读</h1><div class="content"><h2 id="Paper"><a href="#Paper" class="headerlink" title="Paper"></a>Paper</h2><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><blockquote>
<p>我们提出了一种新颖的神经表面重建方法，称为NeuS，用于从2D图像输入中高保真重建对象和场景。现有的神经表面重建方法，如DVR [Niemeyer et al., 2020] 和IDR [Yariv et al., 2020]，需要前景掩码作为监督，容易陷入局部最小值，因此在重建具有严重自遮挡或薄结构的对象时存在困难。与此同时，最近的用于新视角合成的神经方法，如NeRF [Mildenhall et al., 2020]及其变体，使用体积渲染来生成具有鲁棒优化特性的神经场景表示，即使对于高度复杂的对象也是如此。然而，从这种学习到的隐式表示中提取高质量的表面非常困难，因为表示中没有足够的表面约束。在NeuS中，我们提出将表面表示为<strong>有符号距离函数（SDF）的 zero-level-set</strong>，并<strong>开发了一种新的体积渲染方法来训练神经SDF表示</strong>。我们观察到，传统的体积渲染方法会导致表面重建固有的几何误差（即偏差），因此我们<strong>提出了一种在一阶近似中无偏差的新公式，从而即使在没有掩码监督的情况下也能实现更精确的表面重建</strong>。DTU数据集和BlendedMVS数据集上的实验表明，NeuS在高质量表面重建方面优于现有最先进的方法，特别是对于具有复杂结构和自遮挡的对象和场景。</p>
</blockquote>
<h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><blockquote>
<p>从多视角图像重建表面是计算机视觉和计算机图形学中的一个基本问题。具有神经隐式表示的 3D 重建最近已成为传统重建方法的一种非常有前途的替代方案 [37, 8, 2]，因为它具有高重建质量和重建传统方法难以实现的复杂物体的潜力，例如 non-Lambertian surfaces 和 thin structures。最近的研究将表面表示为有符号距离函数 (SDF) [49, 52, 17, 23] 或 occupancy [30, 31]。为了训练其神经模型，这些方法使用可微分的表面渲染方法将 3D 对象渲染为图像，并将其与输入图像进行比较以进行监督。例如，IDR [49] 产生了令人印象深刻的重建结果，但它无法重建具有导致突然深度变化的复杂结构的物体。造成这种限制的原因是 IDR 中使用的表面渲染方法仅考虑每条射线的单个表面交点。因此，梯度仅存在于这个单点，对于有效的反向传播来说，这个点过于局部，当图像深度发生突然变化时，优化会陷入不良的局部最小值。此外，需要对象蒙版作为监督，以收敛到有效的表面。如图 1 (a) 顶部所示，由于洞引起的深度剧烈变化，神经网络会错误地将靠近前表面的点预测为蓝色，从而无法找到远处的蓝色表面。图 1 (b) 中的实际测试示例表明，IDR 无法正确重建具有突然深度变化的边缘附近的表面。</p>
<p>最近，NeRF [29] 及其变体探索使用体积渲染方法来学习体积辐射场，以实现新视图合成。这种体积渲染方法沿每条射线采样多个点，并对采样点的颜色进行 α-composition，以生成用于训练目的的输出像素颜色。体积渲染方法的优点是它可以处理突然的深度变化，因为它考虑了射线上的多个点，因此所有采样点（无论是靠近表面还是在远表面上）都会产生用于反向传播的梯度信号。例如，参考图 1 (a) 底部，当发现近表面（黄色）的颜色与输入图像不一致时，体积渲染方法能够训练网络找到远后表面以产生正确的场景表示。然而，由于它旨在用于新视图合成而不是表面重建，因此 NeRF 仅学习体积密度场，从中提取高质量表面很困难。图 1 (b) 显示了通过 NeRF 学习的密度场的水平集表面。虽然表面正确地解释了突然的深度变化，但它在某些平面区域包含明显的噪声。</p>
<p>在本研究中，我们提出了一种新的神经渲染方案，称为 NeuS，用于多视图表面​​重建。NeuS 使用有符号距离函数 (SDF) 进行表面表示，并使用一种新颖的体积渲染方案来学习神经 SDF 表示。具体而言，通过引入由 SDF 诱导的密度分布，我们可以将体积渲染方法应用于学习隐式 SDF 表示，从而兼具两全其美的优势，即使用神经 SDF 模型进行准确的表面表示，并在体积渲染导致的突然深度变化的情况下进行稳健的网络训练。请注意，简单地将标准体积渲染方法应用于与 SDF 相关的密度会导致重建表面出现明显的偏差（即固有的几何误差）。这是一个新的、重要的观察结果，我们将在后面详细阐述。因此，我们提出了一种新颖的体积渲染方案，以确保在 SDF 的一阶近似中进行无偏表面重建。在 DTU 数据集和 BlendedMVS 数据集上的实验表明，即使没有前景蒙版作为监督，NeuS 也能够重建具有严重遮挡和精细结构的复杂 3D 物体和场景。在重建质量方面，它优于最先进的神经场景表示方法，即 IDR [49] 和 NeRF [29]。</p>
</blockquote>
<p>先 diss 了一下 IDR，直接原因是无法重建深度突变的点。接着讲了下 NeRF，用上了体渲染的方法。</p>
<h3 id="Related-Works"><a href="#Related-Works" class="headerlink" title="Related Works"></a>Related Works</h3><blockquote>
<p>Classical Multi-view Surface and Volumetric Reconstruction. 经典的多视图表面​​和体积重建。传统的多视图三维重建方法大致可分为两类：基于点和表面的重建[2,8,9,37]和体积重建[6,3,38]。基于点和表面的重建方法通过利用图像间光度一致性来估计每个像素的深度图[8]，然后将深度图融合为全局密集点云[26,51]。表面重建通常作为后处理，使用筛选泊松曲面重建[16]等方法。重建质量在很大程度上依赖于对应匹配的质量，而对于没有丰富纹理的物体，匹配对应关系的困难往往会导致重建结果中出现严重的伪影和缺失部分。或者，体积重建方法通过从多视图图像中估计体素网格中的占用率和颜色并评估每个体素的颜色一致性来规避显式对应匹配的困难。由于可实现的体素分辨率有限，这些方法无法实现高精度。</p>
<p>Neural Implicit Representation. 一些方法通过引入归纳偏差来加强深度学习框架中的 3D 理解。这些归纳偏差可以是显式表示，例如体素网格 [13, 5, 47]、点云 [7, 25, 19]、网格 [44, 46, 14] 和隐式表示。神经网络编码的隐式表示最近引起了广泛关注，因为它是连续的并且可以实现高空间分辨率。该表示已成功应用于形状表示 [27, 28, 32, 4, 1, 10, 50, 33]、新视图合成 [40, 24, 15, 29, 22, 34, 35, 43, 39] 和多视图 3D 重建 [49, 30, 17, 12, 23]。<br>我们的工作主要集中于通过经典的渲染技术从二维图像中学习隐式神经表征，该表征编码了三维空间中的几何和外观。受限于这个范围，相关工作可以根据所使用的渲染技术大致分为基于表面渲染的方法和基于体积渲染的方法。基于表面渲染的方法 [30, 17, 49, 23] 假设射线的颜色仅依赖于射线与场景几何图形相交的颜色，这使得梯度仅反向传播到相交点附近的局部区域。因此，这类方法难以重建具有严重自遮挡和突然深度变化的复杂物体。此外，它们通常需要对象蒙版作为监督。相反，我们的方法在这种具有挑战性的情况下表现良好，而不需要蒙版。<br>基于体积渲染的方法，例如 NeRF[29]，通过对每条射线上采样点的颜色进行 α 合成来渲染图像。如介绍中所述，它可以处理突然的深度变化并合成高质量的图像。然而，从学习到的隐式场中提取高保真表面非常困难，因为基于密度的场景表示对其水平集缺乏足够的约束。相反，我们的方法结合了基于表面绘制和基于体积绘制方法的优点，通过将场景空间约束为有符号距离函数，但应用体积绘制来训练这种具有鲁棒性的表示。同时期的工作 UNISURF [31] 也通过体积绘制学习隐式表面。它通过在优化过程中缩小体积绘制的样本区域来提高重建质量。我们的方法与 UNISURF 的不同之处在于，UNISURF 用占用值表示表面，而我们的方法用 SDF 表示场景，因此可以自然地将表面提取为其零级集，从而获得比 UNISURF 更好的重建精度，这将在后面的实验部分看到。</p>
</blockquote>
<h3 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h3><blockquote>
<p>给定一组 3D 对象的 posed image $\left\lbrace\mathcal{I}_k\right\rbrace$，我们的目标是重建其表面 $\mathcal{S}$。表面由神经隐式 SDF 的 zero-level set 表示。为了学习 the weights of the neural network，我们开发了一种新颖的 volume rendering method 来渲染隐式 SDF 中的图像，并最小化渲染图像与输入图像之间的差异。这种体积渲染方法可确保在 NeuS 中进行稳健优化，以重建复杂结构的对象。</p>
</blockquote>
<h4 id="Rendering-Procedure"><a href="#Rendering-Procedure" class="headerlink" title="Rendering Procedure"></a>Rendering Procedure</h4><blockquote>
<p><strong>Scene Representation.</strong> 使用 NeuS，要重建的场景对象由两个函数表示：$f: \mathbb{R}^3 \rightarrow \mathbb{R}$，将空间位置 $\mathbf{x} \in \mathbb{R}^3$ 映射到其与对象的有符号距离，以及 $c: \mathbb{R}^3 \times \mathbb{S}^2 \rightarrow \mathbb{R}^3$，对与点 $\mathbf{x} \in \mathbb{R}^3$ 和 viewing direction 观看方向 $\mathbf{v} \in \mathbb{S}^2$ 相关的颜色进行编码。这两个函数均由多层感知器 (MLP) 编码。对象的表面 $\mathcal{S}$ 由其 SDF 的 zero-level set 表示，即：</p>
<p>$$<br>\mathcal{S}&#x3D;\left\lbrace\mathbf{x} \in \mathbb{R}^3 \mid f(\mathbf{x})&#x3D;0\right\rbrace<br>$$<br>为了使用体渲染方法来训练SDF网络，作者首先引入了一个概率密度函数 $\phi_s(f(\mathbf{x}))$, 称为 $S$-density, 其中 $f(\mathbf{x}), \mathbf{x} \in \mathbb{R}^3$，是 SDF，$\phi_s(x)&#x3D;s e^{-s x} &#x2F;\left(1+e^{-s x}\right)^2$，通常称为 logistic density distribution，是 Sigmoid 函数 $\Phi_s(x)&#x3D;\left(1+e^{-s x}\right)^{-1}$ 的导数，即 $\phi_s(x)&#x3D;\Phi_s^{\prime}(x)$。原则上，$\phi_s(x)$ 可以是任何以 0 为中心的单峰（即钟形）密度分布；这里我们选择逻辑密度分布是因为其计算方便。注意，$\phi_s(x)$  的标准差由 $1 &#x2F; s$ 给出，它也是一个可训练参数，也就是说，随着网络训练收敛，$1 &#x2F; s$ 趋近于零。</p>
<p>直观地看，NeuS 的主要思想是，借助 S-density field $\phi_s(f(\mathbf{x}))$，使用体积渲染来训练 SDF 网络，仅使用 2D 输入图像作为监督。在基于此监督成功最小化损失函数后，网络编码的 SDF 的 zero-level set 有望表示准确重建的表面 $\mathcal{S}$，其诱导的 S-density $\phi_s(f(\mathbf{x}))$ 在表面附近呈现显著的高值。</p>
<p><strong>Rendering.</strong> 为了学习神经 SDF 和 color field 的参数，我们建议使用体积渲染方案来渲染所提出的 SDF 表示中的图像。给定一个像素，我们将从该像素发出的射线表示为 $\lbrace\mathbf{p}(t)&#x3D;\mathbf{o}+t \mathbf{v} \mid t \geq 0\rbrace$，其中 $\mathbf{o}$ 是相机的中心，$\mathbf{v}$ 是射线的单位方向向量。我们通过以下方式累积沿射线的颜色：</p>
<p>$$C(\mathbf{o}, \mathbf{v})&#x3D;\int_0^{+\infty} w(t) c(\mathbf{p}(t), \mathbf{v}) \mathrm{d} t$$</p>
<p>其中 $C(\mathbf{o}, \mathbf{v})$ 是该像素的输出颜色，$w(t)$ 是点 $\mathbf{p}(t)$ 的权重，$c(\mathbf{p}(t), \mathbf{v})$ 是沿视线方向 $\mathbf{v}$ 的点 $\mathbf{p}$ 处的颜色。</p>
<p><strong>Requirements on weight function.</strong> 从二维图像中学习准确的 SDF 表示的关键是在输出颜色和 SDF 之间建立适当的联系，即根据场景的 SDF $f$ 得出射线上适当的权重函数 $w(t)$。下面，我们列出了对权重函数 $w(t)$ 的要求。</p>
<ul>
<li><ol>
<li><strong>Unbiased.</strong> 给定相机射线 $\mathbf{p}(t), w(t)$ 在表面交点 $\mathbf{p}(t^*)$ 处达到局部最大值，即 $f(\mathbf{p}(t^*))&#x3D;0$，即点 $\mathbf{p}(t^*)$ 位于 $\operatorname{SDF}(\mathbf{x})$ 的 zero-level set 上。</li>
</ol>
</li>
<li><ol start="2">
<li><strong>Occlusion-aware.</strong> 给定任意两个深度值 $t_0$ 和 $t_1$，满足 $f\left(t_0\right)&#x3D;f\left(t_1\right), w\left(t_0\right)&gt;0$、$w\left(t_1\right)&gt;0$ 和 $t_0&lt;t_1$，则有 $w\left(t_0\right)&gt;w\left(t_1\right)$。也就是说，当两个点具有相同的 SDF 值（因此具有相同的 SDF 诱导的 S-density value）时，靠近视点的点对最终输出颜色的贡献应该比另一个点更大。</li>
</ol>
</li>
</ul>
<p>无偏权重函数 $w(t)$ 保证相机光线与 SDF 零级集的交点对像素颜色贡献最大。遮挡感知属性确保当光线连续穿过多个表面时，渲染过程将正确使用最靠近相机的表面颜色来计算输出颜色。</p>
<p>接下来，我们将首先介绍一种定义权重函数 $w(t)$ 的简单方法，即直接使用体积渲染的标准管道，并解释为什么它不适合重建，然后再介绍我们对 $w(t)$ 的新构造。</p>
<p><strong>Naive solution.</strong> 为了使权重函数具有遮挡感知能力，一个自然的解决方案是基于标准体积渲染公式 [29]，该公式通过以下方式定义权重函数：</p>
<p>$$w(t)&#x3D;T(t) \sigma(t)$$</p>
<p>其中，$\sigma(t)$ 是经典体积绘制中所谓的体积密度，而 $T(t)&#x3D;$ $\exp (-\int_0^t \sigma(u) \mathrm{d} u)$ 表示沿射线的累积透射率。为采用标准体积密度公式 [29]，这里将 $\sigma(t)$ 设置为等于 S-density value，即 $\sigma(t)&#x3D;\phi_s(f(\mathbf{p}(t)))$，权重函数 $w(t)$ 由公式 3 计算。虽然得到的权重函数具有遮挡感知能力，但它存在偏差，因为它会在重建表面中引入固有误差。如图 2 (a) 所示，权重函数 $w(t)$ 在射线到达表面点 $\mathbf{p}(t^*)$ 之前的某点达到局部最大值，满足 $f(\mathbf{p}(t^*))&#x3D;0$。这一事实将在补充材料中得到证明。</p>
<p><strong>Our solution.</strong> 为了介绍我们的解决方案，我们首先介绍一种构建无偏权重函数的直接方法，该方法直接使用归一化的 S-density 作为权重:</p>
<p>$$w(t)&#x3D;\frac{\phi_s(f(\mathbf{p}(t)))}{\int_0^{+\infty} \phi_s(f(\mathbf{p}(u))) \mathrm{d} u}$$</p>
<p>这种权重函数的构造是无偏的，但不考虑遮挡。例如，如果射线穿透两个表面，SDF 函数 $f$ 在射线上将有两个零点，这会导致权重函数 $w(t)$ 上有两个峰值，并且得到的权重函数将在不考虑遮挡的情况下均匀混合两个表面的颜色。</p>
<p>为此，现在我们将基于上述简单的构造，在 SDF 的一阶近似中设计既具有遮挡感知又无偏的权重函数 $w(t)$。为了确保权重函数 $w(t)$ 具有遮挡感知特性，我们仍将遵循体积渲染的基本框架，如公式 3。然而，与上述简单解决方案中的常规处理不同，我们以一种新的方式从 S-density 定义函数 $w(t)$。我们首先定义一个不透明密度函数 $\rho(t)$，它是标准体积渲染中体积密度 $\sigma$ 的对应项。然后我们通过以下方式计算新的权重函数 $w(t)$:</p>
<p>$$w(t)&#x3D;T(t) \rho(t),\quad \text { where } T(t)&#x3D;\exp (-\int_0^t \rho(u) \mathrm{d} u)$$</p>
<p><strong>How we derive opaque density $\rho$.</strong> 我们首先考虑一个简单的理想情况，其中只有一个表面交点，并且该表面只是一个无限接近相机的平面。由于在此假设下，公式 4 确实满足上述要求，因此我们使用体积渲染的框架推导出与公式 4 的权重定义相对应的底层不透明密度 ρ。然后，我们将此不透明密度推广到多个表面相交的一般情况。</p>
<p>具体而言，在单个平面相交的简单情况下，很容易看出有符号距离函数 $f(\mathbf{p}(t))$ 为 $-|\cos (\theta)| \cdot(t-t^*)$，其中 $f(\mathbf{p}(t^*))&#x3D;0$，$\theta$ 是视线方向 $\mathbf{v}$ 与外表面法向量 $\mathbf{n}$ 之间的角度。由于假设表面为平面，$|\cos (\theta)|$ 为常数。从公式 4 可知:</p>
<p>$$<br>\begin{aligned}<br>w(t) &amp; &#x3D;\lim _{t^* \rightarrow+\infty} \frac{\phi_s(f(\mathbf{p}(t)))}{\int_0^{+\infty} \phi_s(f(\mathbf{p}(u))) \mathrm{d} u} \<br>&amp; &#x3D;\lim _{t^* \rightarrow+\infty} \frac{\phi_s(f(\mathbf{p}(t)))}{\int_0^{+\infty} \phi_s\left(-|\cos (\theta)|\left(u-t^<em>\right)\right) \mathrm{d} u} \<br>&amp; &#x3D;\lim _{t^</em> \rightarrow+\infty} \frac{\phi_s(f(\mathbf{p}(t)))}{\int_{-t^*}^{+\infty} \phi_s\left(-|\cos (\theta)| u^<em>\right) \mathrm{d} u^</em>} \<br>&amp; &#x3D;\lim <em>{t^* \rightarrow+\infty} \frac{\phi_s(f(\mathbf{p}(t)))}{|\cos (\theta)|^{-1} \int</em>{-|\cos (\theta)| t^*}^{+\infty} \phi_s(\hat{u}) \mathrm{d} \hat{u}} \<br>&amp; &#x3D;|\cos (\theta)| \phi_s(f(\mathbf{p}(t))) .<br>\end{aligned}<br>$$</p>
<p>回想一下，体积渲染框架内的权重函数由 $w(t)&#x3D;T(t) \rho(t)$ 给出，其中 $T(t)&#x3D;\exp \left(-\int_0^t \rho(u) \mathrm{d} u\right)$ 表示<em>累积透射率</em>。因此，为了 derive $\rho(t)$，我们有:<br>$$T(t) \rho(t)&#x3D;|\cos (\theta)| \phi_s(f(\mathbf{p}(t)))$$<br>由于 $T(t)&#x3D;\exp \left(-\int_0^t \rho(u) \mathrm{d} u\right)$，因此很容易验证 $T(t) \rho(t)&#x3D;-\frac{\mathrm{d} T}{\mathrm{<del>d} t}(t)$。此外，请注意 $|\cos (\theta)| \phi_s(f(\mathbf{p}(t)))&#x3D;-\frac{\mathrm{d} \Phi_s}{\mathrm{</del>d} t}(f(\mathbf{p}(t)))$。由此可知 $\frac{\mathrm{d} T}{\mathrm{<del>d} t}(t)&#x3D;\frac{\mathrm{d} \Phi_s}{\mathrm{</del>d} t}(f(\mathbf{p}(t)))$。对该等式两边求积分可得出：</p>
</blockquote>
<h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><blockquote>
<p>我们提出了 NeuS，这是一种新的多视图表面​​重建方法，它将 3D 表面表示为神经 SDF，并开发了一种用于训练隐式 SDF 表示的新体积渲染方法。NeuS 可产生高质量的重建，并成功重建具有严重遮挡和复杂结构的物体。它在质量和数量上都优于最先进的技术。我们方法的一个局限性是，尽管我们的方法并不严重依赖纹理特征的对应匹配，但对于无纹理物体，性能仍然会下降（我们在补充材料中展示了失败案例）。此外，NeuS 只有一个尺度参数 s，用于对所有空间位置的概率分布的标准偏差进行建模。因此，一个有趣的未来研究课题是根据不同的局部几何特征对不同空间位置的概率进行建模，同时优化场景表示。负面社会影响：与许多其他基于学习的作品一样，我们的方法需要大量的计算资源进行网络训练，这可能是全球气候变化的一个问题。</p>
</blockquote>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/614008188">NeRF系列工作个人总结 - 氵景页的文章 - 知乎</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/flow_specter/article/details/126222914">论文笔记：NeuS: Learning Neural Implicit Surfaces by Volume Rendering for Multi-view Reconstruction</a></p>
<p><a target="_blank" rel="noopener" href="https://longtimenohack.com/posts/paper_reading/2021_wang_neus/">Jianfei Guo</a></p>
</div><div class="article-licensing box"><div class="licensing-title"><p>NeuS Paper 细读</p><p><a href="https://tosakaucw.github.io/neus-paper-细读/">https://tosakaucw.github.io/neus-paper-细读/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>Author</h6><p>TosakaUCW</p></div></div><div class="level-item is-narrow"><div><h6>Posted on</h6><p>2024-07-04</p></div></div><div class="level-item is-narrow"><div><h6>Updated on</h6><p>2024-07-04</p></div></div><div class="level-item is-narrow"><div><h6>Licensed under</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><!--!--></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/360-gs-paper-%E7%BB%86%E8%AF%BB/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">360-GS Paper 细读</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/epic-institute-of-technology-round-summer-2024-div-1-div-2/"><span class="level-item">EPIC Institute of Technology Round Summer 2024 (Div. 1 + Div. 2)</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">Comments</h3><div id="disqus_thread"><noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div><script>var disqus_config = function () {
            this.page.url = 'https://tosakaucw.github.io/neus-paper-%E7%BB%86%E8%AF%BB/';
            this.page.identifier = 'neus-paper-细读/';
        };
        (function() {
            var d = document, s = d.createElement('script');  
            s.src = '//' + 'tosaka-blog' + '.disqus.com/embed.js';
            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();</script></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1"><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">Catalogue</h3><ul class="menu-list"><li><a class="level is-mobile" href="#Paper"><span class="level-left"><span class="level-item">Paper</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Abstract"><span class="level-left"><span class="level-item">Abstract</span></span></a></li><li><a class="level is-mobile" href="#Introduction"><span class="level-left"><span class="level-item">Introduction</span></span></a></li><li><a class="level is-mobile" href="#Related-Works"><span class="level-left"><span class="level-item">Related Works</span></span></a></li><li><a class="level is-mobile" href="#Method"><span class="level-left"><span class="level-item">Method</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Rendering-Procedure"><span class="level-left"><span class="level-item">Rendering Procedure</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Conclusion"><span class="level-left"><span class="level-item">Conclusion</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Reference"><span class="level-left"><span class="level-item">Reference</span></span></a></li></ul></div></div><style>#toc .menu-list > li > a.is-active + .menu-list { display: block; }#toc .menu-list > li > a + .menu-list { display: none; }</style><script src="/js/toc.js" defer></script></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="TosakaUCW" height="28"></a><p class="is-size-7"><span>&copy; 2024 TosakaUCW</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p><p class="is-size-7">© 2024</p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/TosakaUCW"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script data-pjax src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script data-pjax src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/javascript" id="MathJax-script" async>MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      },
      svg: {
        fontCache: 'global'
      },
      chtml: {
        matchFontHeight: false
      }
    };</script><script src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js"></script><!--!--><!--!--><!--!--><script data-pjax src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>